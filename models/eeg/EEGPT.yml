model_name: EEGPT
modality: EEG
architecture: Transformer
task: Foundation Model
year: 2023
paper_url: https://dl.acm.org/doi/10.5555/3737916.3739155
code_url: https://github.com/BINE022/EEGPT
dataset_tags: [BNCI2014-002]
open_weights: false
notes: "Large pretrained Transformer for EEG trained across diverse EEG corpora with transfer to multiple downstream tasks."
